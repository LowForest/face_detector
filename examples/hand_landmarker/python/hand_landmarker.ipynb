{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LowForest/face_detector/blob/main/examples/hand_landmarker/python/hand_landmarker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_cQX8dWu4Dv"
      },
      "source": [
        "# Hand Landmarks Detection with MediaPipe Tasks\n",
        "\n",
        "This notebook shows you how to use MediaPipe Tasks Python API to detect hand landmarks from images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6PN9FvIx614"
      },
      "source": [
        "## Preparation\n",
        "\n",
        "Let's start with installing MediaPipe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gxbHBsF-8Y_l",
        "outputId": "74c066a3-928e-4ff3-8c59-6bc40fadb4fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping fastai as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping tensorflow-decision-forests as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping ydf as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping spacy as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping thinc as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping opencv-python-headless as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping dopamine-rl as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping albumentations as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping albucore as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: protobuf==4.25.3 in /usr/local/lib/python3.12/dist-packages (4.25.3)\n",
            "Collecting opencv-python==4.8.1.78\n",
            "  Using cached opencv_python-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: mediapipe==0.10.21 in /usr/local/lib/python3.12/dist-packages (0.10.21)\n",
            "Requirement already satisfied: matplotlib<3.9,>=3.7 in /usr/local/lib/python3.12/dist-packages (3.8.4)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.12/dist-packages (0.19.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.21) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.21) (25.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.21) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.21) (0.5.3)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.21) (0.5.3)\n",
            "Collecting opencv-contrib-python (from mediapipe==0.10.21)\n",
            "  Using cached opencv_contrib_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.21) (0.5.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.21) (0.2.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.9,>=3.7) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.9,>=3.7) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.9,>=3.7) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.9,>=3.7) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.9,>=3.7) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.9,>=3.7) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.9,>=3.7) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.9,>=3.7) (2.9.0.post0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16) (0.8.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib<3.9,>=3.7) (1.17.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe==0.10.21) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe==0.10.21) (2.22)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe==0.10.21) (0.5.3)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe==0.10.21) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe==0.10.21) (1.16.1)\n",
            "INFO: pip is looking at multiple versions of opencv-contrib-python to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Using cached opencv_python-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.7 MB)\n",
            "Using cached opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (69.1 MB)\n",
            "Installing collected packages: opencv-python, opencv-contrib-python\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [opencv-contrib-python]\n",
            "\u001b[1A\u001b[2KSuccessfully installed opencv-contrib-python-4.11.0.86 opencv-python-4.8.1.78\n"
          ]
        }
      ],
      "source": [
        "!pip -q uninstall -y fastai tensorflow-decision-forests ydf spacy thinc opencv-python opencv-contrib-python opencv-python-headless dopamine-rl albumentations albucore || true\n",
        "!pip install -U --quiet pip setuptools wheel\n",
        "!pip install \"numpy==1.26.4\" \"protobuf==4.25.3\" \"opencv-python==4.8.1.78\" \"mediapipe==0.10.21\" \"matplotlib>=3.7,<3.9\" \"jedi>=0.16\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a49D7h4TVmru"
      },
      "source": [
        "Then download an off-the-shelf model bundle. Check out the [MediaPipe documentation](https://developers.google.com/mediapipe/solutions/vision/hand_landmarker#models) for more information about this model bundle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OMjuVQiDYJKF"
      },
      "outputs": [],
      "source": [
        "!wget -q https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYKAJ5nDU8-I"
      },
      "source": [
        "## Visualization utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "s3E6NFV-00Qt"
      },
      "outputs": [],
      "source": [
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import os, cv2, time, base64\n",
        "from mediapipe import solutions\n",
        "from mediapipe.tasks import python as mp_python\n",
        "from mediapipe.tasks.python import vision\n",
        "from mediapipe.framework.formats import landmark_pb2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import clear_output, HTML, Javascript, display, update_display\n",
        "from IPython.display import Image as IPyImage\n",
        "from google.colab.output import eval_js\n",
        "\n",
        "MARGIN = 10  # pixels\n",
        "FONT_SIZE = 1\n",
        "FONT_THICKNESS = 1\n",
        "HANDEDNESS_TEXT_COLOR = (88, 205, 54) # vibrant green\n",
        "\n",
        "def draw_landmarks_on_image(rgb_image, detection_result):\n",
        "  hand_landmarks_list = detection_result.hand_landmarks\n",
        "  handedness_list = detection_result.handedness\n",
        "  annotated_image = np.copy(rgb_image)\n",
        "\n",
        "  for idx in range(len(hand_landmarks_list)):\n",
        "    hand_landmarks = hand_landmarks_list[idx]\n",
        "    handedness = handedness_list[idx]\n",
        "\n",
        "    # Draw the hand landmarks.\n",
        "    hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
        "    hand_landmarks_proto.landmark.extend([\n",
        "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z)\n",
        "      for landmark in hand_landmarks\n",
        "    ])\n",
        "    solutions.drawing_utils.draw_landmarks(\n",
        "      annotated_image,\n",
        "      hand_landmarks_proto,\n",
        "      solutions.hands.HAND_CONNECTIONS,\n",
        "      solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
        "      solutions.drawing_styles.get_default_hand_connections_style()\n",
        "    )\n",
        "\n",
        "    # Handedness text\n",
        "    h, w, _ = annotated_image.shape\n",
        "    xs = [lm.x for lm in hand_landmarks]\n",
        "    ys = [lm.y for lm in hand_landmarks]\n",
        "    tx = int(min(xs) * w)\n",
        "    ty = int(min(ys) * h) - MARGIN\n",
        "    ty = max(ty, 20)  # tránh tràn trên\n",
        "    cv2.putText(annotated_image, f\"{handedness[0].category_name}\",\n",
        "                (tx, ty), cv2.FONT_HERSHEY_DUPLEX,\n",
        "                FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n",
        "  return annotated_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83PEJNp9yPBU"
      },
      "source": [
        "## Download test image\n",
        "\n",
        "Let's grab a test image that we'll use later. The image is from [Unsplash](https://unsplash.com/photos/mt2fyrdXxzk)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-skLwMBmMN_"
      },
      "source": [
        "Optionally, you can upload your own image. If you want to do so, uncomment and run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "etBjSdwImQPw"
      },
      "outputs": [],
      "source": [
        "def overlay_hud(bgr_image, hands_count, fps):\n",
        "  text = f\"Hands: {hands_count} | FPS: {fps:.1f}\"\n",
        "  cv2.putText(bgr_image, text, (10, 24), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "              0.7, (0, 255, 0), 2, cv2.LINE_AA)\n",
        "  return bgr_image\n",
        "\n",
        "def data_url_to_bgr(data_url: str):\n",
        "  \"\"\"Convert JS canvas dataURL -> OpenCV BGR image.\"\"\"\n",
        "  if not data_url:\n",
        "    return None\n",
        "  header, encoded = data_url.split(',', 1)\n",
        "  np_data = np.frombuffer(base64.b64decode(encoded), np.uint8)\n",
        "  bgr = cv2.imdecode(np_data, cv2.IMREAD_COLOR)\n",
        "  return bgr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy4r2_ePylIa"
      },
      "source": [
        "## Running inference and visualizing the results\n",
        "\n",
        "Here are the steps to run hand landmark detection using MediaPipe.\n",
        "\n",
        "Check out the [MediaPipe documentation](https://developers.google.com/mediapipe/solutions/vision/hand_landmarker/python) to learn more about configuration options that this solution supports.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_JVO3rvPD4RN",
        "outputId": "8ba9cdd1-5f89-4992-99bd-58d2e316d7d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HandLandmarker ready (VIDEO mode).\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists('hand_landmarker.task'):\n",
        "    raise FileNotFoundError(\"Thiếu hand_landmarker.task — chạy lại Cell 1.\")\n",
        "\n",
        "base_options = mp_python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
        "options = vision.HandLandmarkerOptions(\n",
        "    base_options=base_options,\n",
        "    num_hands=2,\n",
        "    running_mode=vision.RunningMode.VIDEO\n",
        ")\n",
        "detector = vision.HandLandmarker.create_from_options(options)\n",
        "print(\"HandLandmarker ready (VIDEO mode).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SE6_sPCXaX3g",
        "outputId": "553a7f8b-ae5e-4ff5-e915-935f15a188f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<div id=\"snap-wrap\" style=\"text-align:center;margin:8px 0;font-family:system-ui,sans-serif\">\n",
              "  <video id=\"snap-video\" autoplay playsinline muted\n",
              "         style=\"width:480px;height:360px;background:#000;border-radius:8px;object-fit:cover;display:block;margin:0 auto\"></video>\n",
              "  <div style=\"margin-top:8px\">\n",
              "    <button id=\"snap-start\" style=\"padding:6px 12px\">Start</button>\n",
              "    <button id=\"snap-capture\" style=\"padding:6px 12px\">Capture</button>\n",
              "    <button id=\"snap-stop\" style=\"padding:6px 12px\">Stop</button>\n",
              "    <span id=\"snap-status\" style=\"margin-left:10px;color:#0a8\">idle</span>\n",
              "  </div>\n",
              "</div>\n",
              "<script>\n",
              "(() => {\n",
              "  const v = document.getElementById('snap-video');\n",
              "  const st = document.getElementById('snap-status');\n",
              "  let stream = null;\n",
              "\n",
              "  async function start() {\n",
              "    try {\n",
              "      if (stream) stop();\n",
              "      stream = await navigator.mediaDevices.getUserMedia({video:{width:{ideal:480},height:{ideal:360}}, audio:false});\n",
              "      v.srcObject = stream;\n",
              "      await v.play();\n",
              "      st.textContent = 'live'; st.style.color = '#0a8';\n",
              "      return true;\n",
              "    } catch(e){ console.error(e); st.textContent='permission error'; st.style.color='#c33'; return false; }\n",
              "  }\n",
              "  function stop() {\n",
              "    if(stream){ stream.getTracks().forEach(t=>t.stop()); stream=null; }\n",
              "    v.srcObject=null; st.textContent='stopped'; st.style.color='#888'; return true;\n",
              "  }\n",
              "  function capture(q=0.9){\n",
              "    if(!stream) return null;\n",
              "    const c=document.createElement('canvas'); c.width=480; c.height=360;\n",
              "    const ctx=c.getContext('2d'); ctx.drawImage(v,0,0,c.width,c.height);\n",
              "    return c.toDataURL('image/jpeg', q);\n",
              "  }\n",
              "  document.getElementById('snap-start').onclick = start;\n",
              "  document.getElementById('snap-stop').onclick  = stop;\n",
              "  document.getElementById('snap-capture').onclick = ()=>{};\n",
              "  // expose locally for this cell\n",
              "  window._snap = { start, stop, capture };\n",
              "})();\n",
              "</script>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chưa live. Bấm Start rồi chạy lại 2 dòng cuối trong cell để Capture.\n"
          ]
        }
      ],
      "source": [
        "HTML_SNAP = r\"\"\"\n",
        "<div id=\"snap-wrap\" style=\"text-align:center;margin:8px 0;font-family:system-ui,sans-serif\">\n",
        "  <video id=\"snap-video\" autoplay playsinline muted\n",
        "         style=\"width:480px;height:360px;background:#000;border-radius:8px;object-fit:cover;display:block;margin:0 auto\"></video>\n",
        "  <div style=\"margin-top:8px\">\n",
        "    <button id=\"snap-start\" style=\"padding:6px 12px\">Start</button>\n",
        "    <button id=\"snap-capture\" style=\"padding:6px 12px\">Capture</button>\n",
        "    <button id=\"snap-stop\" style=\"padding:6px 12px\">Stop</button>\n",
        "    <span id=\"snap-status\" style=\"margin-left:10px;color:#0a8\">idle</span>\n",
        "  </div>\n",
        "</div>\n",
        "<script>\n",
        "(() => {\n",
        "  const v = document.getElementById('snap-video');\n",
        "  const st = document.getElementById('snap-status');\n",
        "  let stream = null;\n",
        "\n",
        "  async function start() {\n",
        "    try {\n",
        "      if (stream) stop();\n",
        "      stream = await navigator.mediaDevices.getUserMedia({video:{width:{ideal:480},height:{ideal:360}}, audio:false});\n",
        "      v.srcObject = stream;\n",
        "      await v.play();\n",
        "      st.textContent = 'live'; st.style.color = '#0a8';\n",
        "      return true;\n",
        "    } catch(e){ console.error(e); st.textContent='permission error'; st.style.color='#c33'; return false; }\n",
        "  }\n",
        "  function stop() {\n",
        "    if(stream){ stream.getTracks().forEach(t=>t.stop()); stream=null; }\n",
        "    v.srcObject=null; st.textContent='stopped'; st.style.color='#888'; return true;\n",
        "  }\n",
        "  function capture(q=0.9){\n",
        "    if(!stream) return null;\n",
        "    const c=document.createElement('canvas'); c.width=480; c.height=360;\n",
        "    const ctx=c.getContext('2d'); ctx.drawImage(v,0,0,c.width,c.height);\n",
        "    return c.toDataURL('image/jpeg', q);\n",
        "  }\n",
        "  document.getElementById('snap-start').onclick = start;\n",
        "  document.getElementById('snap-stop').onclick  = stop;\n",
        "  document.getElementById('snap-capture').onclick = ()=>{};\n",
        "  // expose locally for this cell\n",
        "  window._snap = { start, stop, capture };\n",
        "})();\n",
        "</script>\n",
        "\"\"\"\n",
        "display(HTML(HTML_SNAP))\n",
        "\n",
        "# tự thử start (nếu bị chặn, bấm Start rồi chạy lại 2 dòng dưới)\n",
        "_ = eval_js(\"window._snap.start()\")\n",
        "data_url = eval_js(\"window._snap.capture(0.9)\")\n",
        "if data_url:\n",
        "    from IPython.display import display\n",
        "    import cv2\n",
        "    bgr = data_url_to_bgr(data_url)\n",
        "    ok, buf = cv2.imencode('.jpg', bgr, [cv2.IMWRITE_JPEG_QUALITY, 90])\n",
        "    display(IPyImage(data=buf.tobytes()))\n",
        "else:\n",
        "    print(\"Chưa live. Bấm Start rồi chạy lại 2 dòng cuối trong cell để Capture.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WIDGET = r\"\"\"\n",
        "<div id=\"rt-wrap\" style=\"text-align:center; margin:8px 0; font-family:system-ui,sans-serif\">\n",
        "  <video id=\"rt-video\" autoplay playsinline muted\n",
        "         style=\"position:absolute; left:-9999px; width:1px; height:1px; opacity:0;\"></video>\n",
        "  <div style=\"margin-top:8px\">\n",
        "    <button id=\"rt-start\" style=\"padding:6px 12px\">Start</button>\n",
        "    <button id=\"rt-stop\"  style=\"padding:6px 12px\">Stop</button>\n",
        "    <span id=\"rt-status\" style=\"margin-left:10px;color:#0a8\">idle</span>\n",
        "  </div>\n",
        "</div>\n",
        "<script>\n",
        "(() => {\n",
        "  const v  = document.getElementById('rt-video');\n",
        "  const st = document.getElementById('rt-status');\n",
        "  let stream = null;\n",
        "\n",
        "  async function startWebcam(){\n",
        "    try{\n",
        "      if(stream) stopWebcam();\n",
        "      stream = await navigator.mediaDevices.getUserMedia({\n",
        "        video:{ width:{ideal:640}, height:{ideal:480} }, audio:false\n",
        "      });\n",
        "      v.srcObject = stream;\n",
        "      await v.play();\n",
        "      st.textContent='live'; st.style.color='#0a8';\n",
        "      return true;\n",
        "    }catch(e){\n",
        "      console.error(e);\n",
        "      st.textContent='permission error'; st.style.color='#c33';\n",
        "      return false;\n",
        "    }\n",
        "  }\n",
        "  function stopWebcam(){\n",
        "    if(stream){ stream.getTracks().forEach(t=>t.stop()); stream=null; }\n",
        "    v.srcObject=null; st.textContent='stopped'; st.style.color='#888';\n",
        "    return true;\n",
        "  }\n",
        "  function isStreaming(){\n",
        "    return !!(stream && stream.getVideoTracks().length &&\n",
        "              stream.getVideoTracks()[0].readyState==='live');\n",
        "  }\n",
        "  function captureFrame(q=0.75){\n",
        "    if(!isStreaming()) return null;\n",
        "    const c=document.createElement('canvas'); c.width=640; c.height=480;\n",
        "    const ctx=c.getContext('2d'); ctx.drawImage(v,0,0,c.width,c.height);\n",
        "    return c.toDataURL('image/jpeg', q);\n",
        "  }\n",
        "\n",
        "  document.getElementById('rt-start').onclick = startWebcam;\n",
        "  document.getElementById('rt-stop').onclick  = stopWebcam;\n",
        "  window._rt = { startWebcam, stopWebcam, isStreaming, captureFrame };\n",
        "})();\n",
        "</script>\n",
        "\"\"\"\n",
        "display(HTML(WIDGET))\n",
        "\n",
        "# 5B) Auto-start; nếu bị chặn quyền, bấm Start rồi chạy lại cell\n",
        "_ = eval_js(\"window._rt.startWebcam()\")\n",
        "\n",
        "# Chờ webcam live\n",
        "t0 = time.perf_counter()\n",
        "while True:\n",
        "    live = eval_js(\"window._rt.isStreaming && window._rt.isStreaming()\")\n",
        "    if live or (time.perf_counter() - t0 > 12):\n",
        "        break\n",
        "    time.sleep(0.2)\n",
        "if not live:\n",
        "    raise RuntimeError(\"Camera chưa live trong Cell 5. Bấm **Start** rồi chạy lại cell.\")\n",
        "\n",
        "# 5C) Tạo detector NGAY TRONG CELL 5 (tránh dính timestamp cũ)\n",
        "base_opts = mp_python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
        "detector = vision.HandLandmarker.create_from_options(vision.HandLandmarkerOptions(\n",
        "    base_options=base_opts,\n",
        "    num_hands=2,\n",
        "    running_mode=vision.RunningMode.VIDEO\n",
        "))\n",
        "\n",
        "# === helper: hiển thị khung đen khi dừng ===\n",
        "def show_black_frame(display_handle, w=640, h=480, text=\"\"):\n",
        "    black = np.zeros((h, w, 3), dtype=np.uint8)\n",
        "    if text:\n",
        "        cv2.putText(black, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (200,200,200), 2, cv2.LINE_AA)\n",
        "    ok, buf = cv2.imencode('.jpg', black, [cv2.IMWRITE_JPEG_QUALITY, 80])\n",
        "    img_obj = IPyImage(data=buf.tobytes())\n",
        "    if display_handle is None:\n",
        "        return display(img_obj, display_id=True)\n",
        "    else:\n",
        "        update_display(img_obj, display_id=display_handle.display_id)\n",
        "        return display_handle\n",
        "\n",
        "# 5D) Vòng lặp realtime (timestamp đơn điệu + đen màn khi Stop)\n",
        "target_fps = 24\n",
        "frame_dt = 1.0 / target_fps\n",
        "alpha, fps = 0.9, 0.0\n",
        "t_prev = time.perf_counter()\n",
        "t_start = t_prev\n",
        "display_handle = None\n",
        "last_w, last_h = 640, 480\n",
        "\n",
        "print(\"Realtime running... (nhấn Stop để dừng)\")\n",
        "\n",
        "try:\n",
        "    while True:\n",
        "        # Nếu người dùng ấn Stop: hiển thị khung đen rồi thoát\n",
        "        if not eval_js(\"window._rt.isStreaming()\"):\n",
        "            display_handle = show_black_frame(display_handle, last_w, last_h, \"Camera stopped\")\n",
        "            print(\"Webcam stopped.\")\n",
        "            break\n",
        "\n",
        "        data_url = eval_js(\"window._rt.captureFrame(0.75)\")\n",
        "        if data_url is None:\n",
        "            time.sleep(0.02)\n",
        "            continue\n",
        "\n",
        "        # Decode base64 -> BGR\n",
        "        header, encoded = data_url.split(',', 1)\n",
        "        b = np.frombuffer(base64.b64decode(encoded), np.uint8)\n",
        "        bgr = cv2.imdecode(b, cv2.IMREAD_COLOR)\n",
        "        if bgr is None:\n",
        "            continue\n",
        "\n",
        "        # Cập nhật kích thước mới nhất (để khi stop hiển thị đen đúng size)\n",
        "        last_h, last_w = bgr.shape[:2]\n",
        "\n",
        "        # -> RGB & MediaPipe Image\n",
        "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
        "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb)\n",
        "\n",
        "        # timestamp đơn điệu (ms)\n",
        "        ts_ms = int((time.perf_counter() - t_start) * 1000)\n",
        "\n",
        "        # Detect\n",
        "        result = detector.detect_for_video(mp_image, ts_ms)\n",
        "\n",
        "        # Vẽ overlay\n",
        "        annotated = draw_landmarks_on_image(rgb, result)\n",
        "        out_bgr = cv2.cvtColor(annotated, cv2.COLOR_RGB2BGR)\n",
        "        hands_count = len(result.hand_landmarks)\n",
        "\n",
        "        # FPS EMA\n",
        "        t_now = time.perf_counter()\n",
        "        inst_fps = 1.0 / max(t_now - t_prev, 1e-6)\n",
        "        fps = alpha * fps + (1 - alpha) * inst_fps\n",
        "        t_prev = t_now\n",
        "\n",
        "        # HUD\n",
        "        cv2.putText(out_bgr, f\"Hands: {hands_count} | FPS: {fps:.1f}\",\n",
        "                    (10, 24), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2, cv2.LINE_AA)\n",
        "\n",
        "        ok, buf = cv2.imencode('.jpg', out_bgr, [cv2.IMWRITE_JPEG_QUALITY, 80])\n",
        "        if not ok:\n",
        "            continue\n",
        "        img_obj = IPyImage(data=buf.tobytes())\n",
        "        if display_handle is None:\n",
        "            display_handle = display(img_obj, display_id=True)\n",
        "        else:\n",
        "            update_display(img_obj, display_id=display_handle.display_id)\n",
        "\n",
        "        # pace theo target_fps\n",
        "        time.sleep(max(0.0, frame_dt - (time.perf_counter() - t_now)))\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    # Người dùng dừng thủ công: cũng hiển thị khung đen\n",
        "    display_handle = show_black_frame(display_handle, last_w, last_h, \"Interrupted\")\n",
        "    print(\"Interrupted.\")\n",
        "finally:\n",
        "    try:\n",
        "        detector.close()\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        _ = eval_js(\"window._rt.stopWebcam()\")\n",
        "    except Exception:\n",
        "        pass"
      ],
      "metadata": {
        "id": "soDQ5rzjDjga",
        "outputId": "68df7116-363b-43ab-a9d0-6ec73e5aa5f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<div id=\"rt-wrap\" style=\"text-align:center; margin:8px 0; font-family:system-ui,sans-serif\">\n",
              "  <video id=\"rt-video\" autoplay playsinline muted\n",
              "         style=\"position:absolute; left:-9999px; width:1px; height:1px; opacity:0;\"></video>\n",
              "  <div style=\"margin-top:8px\">\n",
              "    <button id=\"rt-start\" style=\"padding:6px 12px\">Start</button>\n",
              "    <button id=\"rt-stop\"  style=\"padding:6px 12px\">Stop</button>\n",
              "    <span id=\"rt-status\" style=\"margin-left:10px;color:#0a8\">idle</span>\n",
              "  </div>\n",
              "</div>\n",
              "<script>\n",
              "(() => {\n",
              "  const v  = document.getElementById('rt-video');\n",
              "  const st = document.getElementById('rt-status');\n",
              "  let stream = null;\n",
              "\n",
              "  async function startWebcam(){\n",
              "    try{\n",
              "      if(stream) stopWebcam();\n",
              "      stream = await navigator.mediaDevices.getUserMedia({\n",
              "        video:{ width:{ideal:640}, height:{ideal:480} }, audio:false\n",
              "      });\n",
              "      v.srcObject = stream;\n",
              "      await v.play();\n",
              "      st.textContent='live'; st.style.color='#0a8';\n",
              "      return true;\n",
              "    }catch(e){\n",
              "      console.error(e);\n",
              "      st.textContent='permission error'; st.style.color='#c33';\n",
              "      return false;\n",
              "    }\n",
              "  }\n",
              "  function stopWebcam(){\n",
              "    if(stream){ stream.getTracks().forEach(t=>t.stop()); stream=null; }\n",
              "    v.srcObject=null; st.textContent='stopped'; st.style.color='#888';\n",
              "    return true;\n",
              "  }\n",
              "  function isStreaming(){\n",
              "    return !!(stream && stream.getVideoTracks().length &&\n",
              "              stream.getVideoTracks()[0].readyState==='live');\n",
              "  }\n",
              "  function captureFrame(q=0.75){\n",
              "    if(!isStreaming()) return null;\n",
              "    const c=document.createElement('canvas'); c.width=640; c.height=480;\n",
              "    const ctx=c.getContext('2d'); ctx.drawImage(v,0,0,c.width,c.height);\n",
              "    return c.toDataURL('image/jpeg', q);\n",
              "  }\n",
              "\n",
              "  document.getElementById('rt-start').onclick = startWebcam;\n",
              "  document.getElementById('rt-stop').onclick  = stopWebcam;\n",
              "  window._rt = { startWebcam, stopWebcam, isStreaming, captureFrame };\n",
              "})();\n",
              "</script>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Camera chưa live trong Cell 5. Bấm **Start** rồi chạy lại cell.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3558143242.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Camera chưa live trong Cell 5. Bấm **Start** rồi chạy lại cell.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# 5C) Tạo detector NGAY TRONG CELL 5 (tránh dính timestamp cũ)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Camera chưa live trong Cell 5. Bấm **Start** rồi chạy lại cell."
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "h2q27gKz1H20"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}