{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LowForest/face_detector/blob/main/examples/face_landmarker/python/%5BMediaPipe_Python_Tasks%5D_Face_Landmarker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2q27gKz1H20"
      },
      "source": [
        "##### Copyright 2023 The MediaPipe Authors. All Rights Reserved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_cQX8dWu4Dv"
      },
      "source": [
        "# Face Landmarks Detection with MediaPipe Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6PN9FvIx614"
      },
      "source": [
        "## Preparation\n",
        "\n",
        "Let's start with installing MediaPipe."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip uninstall -y -q dopamine-rl albumentations albucore ydf grpcio-status spacy thinc opencv-python opencv-contrib-python opencv-python-headless || true\n",
        "!pip install -U --quiet pip setuptools wheel\n",
        "!pip install --quiet \"numpy==1.26.4\" \"protobuf==4.25.3\" \"opencv-python==4.8.1.78\" \"mediapipe==0.10.21\" \"matplotlib>=3.7,<3.9\" \"jedi>=0.16\""
      ],
      "metadata": {
        "id": "Ic8Msb3E9viI",
        "outputId": "4d101225-09ec-4eb2-d4ec-ea128060653a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping dopamine-rl as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping albumentations as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping albucore as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping ydf as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping grpcio-status as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping spacy as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping thinc as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping opencv-python-headless as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a49D7h4TVmru"
      },
      "source": [
        "Then download the off-the-shelf model bundle(s). Check out the [MediaPipe documentation](https://developers.google.com/mediapipe/solutions/vision/face_landmarker#models) for more information about these model bundles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OMjuVQiDYJKF"
      },
      "outputs": [],
      "source": [
        "!wget -O face_landmarker_v2_with_blendshapes.task -q https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYKAJ5nDU8-I"
      },
      "source": [
        "## Visualization utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "s3E6NFV-00Qt"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import mediapipe as mp\n",
        "import os, time, base64, types, sys, cv2, numpy as np\n",
        "from mediapipe import solutions\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "from mediapipe.framework.formats import landmark_pb2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab.output import eval_js\n",
        "from IPython.display import display, Image as IPyImage\n",
        "\n",
        "\n",
        "def draw_landmarks_on_image(rgb_image, detection_result):\n",
        "  face_landmarks_list = detection_result.face_landmarks\n",
        "  annotated_image = np.copy(rgb_image)\n",
        "\n",
        "  # Loop through the detected faces to visualize.\n",
        "  for idx in range(len(face_landmarks_list)):\n",
        "    face_landmarks = face_landmarks_list[idx]\n",
        "\n",
        "    # Draw the face landmarks.\n",
        "    face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
        "    face_landmarks_proto.landmark.extend([\n",
        "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in face_landmarks\n",
        "    ])\n",
        "\n",
        "    solutions.drawing_utils.draw_landmarks(\n",
        "        image=annotated_image,\n",
        "        landmark_list=face_landmarks_proto,\n",
        "        connections=mp.solutions.face_mesh.FACEMESH_TESSELATION,\n",
        "        landmark_drawing_spec=None,\n",
        "        connection_drawing_spec=mp.solutions.drawing_styles\n",
        "        .get_default_face_mesh_tesselation_style())\n",
        "    solutions.drawing_utils.draw_landmarks(\n",
        "        image=annotated_image,\n",
        "        landmark_list=face_landmarks_proto,\n",
        "        connections=mp.solutions.face_mesh.FACEMESH_CONTOURS,\n",
        "        landmark_drawing_spec=None,\n",
        "        connection_drawing_spec=mp.solutions.drawing_styles\n",
        "        .get_default_face_mesh_contours_style())\n",
        "    solutions.drawing_utils.draw_landmarks(\n",
        "        image=annotated_image,\n",
        "        landmark_list=face_landmarks_proto,\n",
        "        connections=mp.solutions.face_mesh.FACEMESH_IRISES,\n",
        "          landmark_drawing_spec=None,\n",
        "          connection_drawing_spec=mp.solutions.drawing_styles\n",
        "          .get_default_face_mesh_iris_connections_style())\n",
        "\n",
        "  return annotated_image\n",
        "\n",
        "def plot_face_blendshapes_bar_graph(face_blendshapes):\n",
        "  # Extract the face blendshapes category names and scores.\n",
        "  face_blendshapes_names = [face_blendshapes_category.category_name for face_blendshapes_category in face_blendshapes]\n",
        "  face_blendshapes_scores = [face_blendshapes_category.score for face_blendshapes_category in face_blendshapes]\n",
        "  # The blendshapes are ordered in decreasing score value.\n",
        "  face_blendshapes_ranks = range(len(face_blendshapes_names))\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(12, 12))\n",
        "  bar = ax.barh(face_blendshapes_ranks, face_blendshapes_scores, label=[str(x) for x in face_blendshapes_ranks])\n",
        "  ax.set_yticks(face_blendshapes_ranks, face_blendshapes_names)\n",
        "  ax.invert_yaxis()\n",
        "\n",
        "  # Label each bar with values\n",
        "  for score, patch in zip(face_blendshapes_scores, bar.patches):\n",
        "    plt.text(patch.get_x() + patch.get_width(), patch.get_y(), f\"{score:.4f}\", va=\"top\")\n",
        "\n",
        "  ax.set_xlabel('Score')\n",
        "  ax.set_title(\"Face Blendshapes\")\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy4r2_ePylIa"
      },
      "source": [
        "## Running inference and visualizing the results\n",
        "\n",
        "Here are the steps to run face landmark detection using MediaPipe.\n",
        "\n",
        "Check out the [MediaPipe documentation](https://developers.google.com/mediapipe/solutions/vision/face_landmarker/python) to learn more about configuration options that this task supports.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_JVO3rvPD4RN",
        "outputId": "32c63115-fcc0-4ac8-84bb-7deb518b8727",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "NotFoundError: Requested device not found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4088044625.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# --- 1) Bật webcam & xin quyền\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"window._mp.initCam()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# --- 2) Chọn model .task đã tải\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: NotFoundError: Requested device not found"
          ]
        }
      ],
      "source": [
        "# --- 0) JS: khởi tạo webcam + hàm lấy frame, tất cả trong window._mp\n",
        "eval_js(r\"\"\"\n",
        "(() => {\n",
        "  if (!window._mp) window._mp = {};\n",
        "  window._mp.initCam = async () => {\n",
        "    if (window._mp.stream && window._mp.stream.active) return true;\n",
        "\n",
        "    // Khối UI riêng cho webcam\n",
        "    const wrap = document.createElement('div'); wrap.id = '_mp_wrap';\n",
        "    const video = document.createElement('video'); video.style.maxWidth = '100%';\n",
        "    const stop  = document.createElement('button'); stop.textContent = '⏹ Stop'; stop.style.margin = '6px 0';\n",
        "    wrap.appendChild(video); wrap.appendChild(document.createElement('br')); wrap.appendChild(stop);\n",
        "    document.body.appendChild(wrap);\n",
        "\n",
        "    const stream = await navigator.mediaDevices.getUserMedia({\n",
        "      video: {width: {ideal: 960}, height: {ideal: 540}}, audio: false\n",
        "    });\n",
        "    video.srcObject = stream; await video.play();\n",
        "\n",
        "    const canvas = document.createElement('canvas');\n",
        "    const ctx = canvas.getContext('2d');\n",
        "\n",
        "    stop.onclick = () => { stream.getTracks().forEach(t => t.stop()); };\n",
        "\n",
        "    window._mp.wrap = wrap;\n",
        "    window._mp.video = video;\n",
        "    window._mp.stream = stream;\n",
        "    window._mp.canvas = canvas;\n",
        "    window._mp.ctx = ctx;\n",
        "    return true;\n",
        "  };\n",
        "\n",
        "  window._mp.camActive = () => !!(window._mp && window._mp.stream && window._mp.stream.active);\n",
        "\n",
        "  window._mp.captureFrame = (q=0.85) => {\n",
        "    if (!window._mp.camActive()) return null;\n",
        "    const {video, canvas, ctx} = window._mp;\n",
        "    if (!video.videoWidth || !video.videoHeight) return null;\n",
        "    canvas.width = video.videoWidth; canvas.height = video.videoHeight;\n",
        "    ctx.drawImage(video, 0, 0);\n",
        "    return canvas.toDataURL('image/jpeg', q); // dataURL JPEG\n",
        "  };\n",
        "\n",
        "  window._mp.stopCam = () => {\n",
        "    if (window._mp && window._mp.stream) window._mp.stream.getTracks().forEach(t=>t.stop());\n",
        "    return true;\n",
        "  };\n",
        "  return true;\n",
        "})()\n",
        "\"\"\")\n",
        "\n",
        "# --- 1) Bật webcam & xin quyền\n",
        "_ = eval_js(\"window._mp.initCam()\")\n",
        "\n",
        "# --- 2) Chọn model .task đã tải\n",
        "MODEL_CANDIDATES = [\"face_landmarker_v2_with_blendshapes.task\", \"face_landmarker.task\"]\n",
        "MODEL_PATH = next((p for p in MODEL_CANDIDATES if os.path.exists(p)), None)\n",
        "assert MODEL_PATH, \"Không thấy file model .task (ví dụ: face_landmarker_v2_with_blendshapes.task).\"\n",
        "\n",
        "# --- 3) Tạo Face Landmarker ở VIDEO mode (giữ blendshapes/transform như bạn dùng)\n",
        "base_vid = python.BaseOptions(model_asset_path=MODEL_PATH)\n",
        "options_vid = vision.FaceLandmarkerOptions(\n",
        "    base_options=base_vid,\n",
        "    running_mode=vision.RunningMode.VIDEO,\n",
        "    num_faces=1,\n",
        "    output_face_blendshapes=True,\n",
        "    output_facial_transformation_matrixes=True,\n",
        "    min_face_detection_confidence=0.5,\n",
        "    min_face_presence_confidence=0.5,\n",
        "    min_tracking_confidence=0.5,\n",
        ")\n",
        "detector_stream = vision.FaceLandmarker.create_from_options(options_vid)\n",
        "\n",
        "# --- 4) Tùy chọn ghi video ra file\n",
        "RECORD = True\n",
        "OUTPUT_FILE = \"face_stream.mp4\"\n",
        "FOURCC = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "TARGET_FPS = 15.0\n",
        "writer = None\n",
        "\n",
        "# --- 5) Tạo DisplayHandle để cập nhật ảnh đã annotate mà không clear_output\n",
        "disp = display(IPyImage(data=b\"\", format='png'), display_id=True)\n",
        "\n",
        "# --- 6) Loop cho đến khi bạn bấm ⏹ Stop\n",
        "t_prev = time.perf_counter()\n",
        "ts_ms = 0\n",
        "log_prev = 0.0\n",
        "mat_prev = 0.0\n",
        "\n",
        "try:\n",
        "  while eval_js(\"(window._mp && window._mp.camActive) ? window._mp.camActive() : false\"):\n",
        "    data = eval_js(\"(window._mp && window._mp.captureFrame) ? window._mp.captureFrame(0.8) : null\")\n",
        "    if not data:\n",
        "      time.sleep(0.01)\n",
        "      continue\n",
        "\n",
        "    # dataURL -> np.uint8 -> BGR\n",
        "    b64 = data.split(\",\")[1]\n",
        "    buf = np.frombuffer(base64.b64decode(b64), dtype=np.uint8)\n",
        "    frame_bgr = cv2.imdecode(buf, cv2.IMREAD_COLOR)\n",
        "    if frame_bgr is None:\n",
        "      continue\n",
        "\n",
        "    # BGR -> RGB (SRGB) cho MediaPipe\n",
        "    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
        "    mp_img = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)\n",
        "\n",
        "    # Timestamp tăng dần (bắt buộc VIDEO mode)\n",
        "    now = time.perf_counter()\n",
        "    dt_ms = max(1, int((now - t_prev) * 1000))\n",
        "    ts_ms += dt_ms\n",
        "    t_prev = now\n",
        "\n",
        "    # Detect + vẽ bằng hàm của bạn (mesh/contours/iris)\n",
        "    result = detector_stream.detect_for_video(mp_img, ts_ms)\n",
        "    # --- ĐẾM SỐ ĐIỂM / SỐ MẶT ---\n",
        "    faces = result.face_landmarks or []\n",
        "    num_faces = len(faces)\n",
        "    pts_each = [len(lmks) for lmks in faces]             # thường ~478 điểm/face\n",
        "    total_pts = sum(pts_each)\n",
        "\n",
        "    # Ghi overlay lên khung hình (khi đã có annotated_rgb / annotated_bgr)\n",
        "    # (giữ nguyên style vẽ của bạn dùng draw_landmarks_on_image)\n",
        "    annotated_rgb = draw_landmarks_on_image(frame_rgb, result)\n",
        "    annotated_bgr = cv2.cvtColor(annotated_rgb, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    label = f\"faces: {num_faces} | pts(each): {pts_each} | total: {total_pts}\"\n",
        "    cv2.putText(annotated_bgr, label, (10, 28),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
        "\n",
        "    # Throttle in ra console mỗi ~1s để tránh spam\n",
        "    now_s = time.perf_counter()\n",
        "    if now_s - log_prev > 1.0:\n",
        "        print(label)\n",
        "        log_prev = now_s\n",
        "\n",
        "    annotated_rgb = draw_landmarks_on_image(frame_rgb, result)   # <-- giữ nguyên style của bạn\n",
        "    annotated_bgr = cv2.cvtColor(annotated_rgb, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Ghi file nếu bật RECORD\n",
        "    if RECORD:\n",
        "      if writer is None:\n",
        "        h, w = annotated_bgr.shape[:2]\n",
        "        writer = cv2.VideoWriter(OUTPUT_FILE, FOURCC, TARGET_FPS, (w, h))\n",
        "      writer.write(annotated_bgr)\n",
        "\n",
        "    # Cập nhật preview (không clear_output)\n",
        "    _, enc = cv2.imencode('.jpg', annotated_bgr)\n",
        "    disp.update(IPyImage(data=enc.tobytes()))\n",
        "finally:\n",
        "  if writer is not None:\n",
        "    writer.release()\n",
        "  _ = eval_js(\"(window._mp && window._mp.stopCam) ? (window._mp.stopCam(), true) : true\")\n",
        "  print(f\"Stopped. Video saved to: {os.path.abspath(OUTPUT_FILE) if RECORD else '(recording disabled)'}\")\n",
        "\n",
        "  # --- IN RA FACIAL TRANSFORMATION MATRIXES ---\n",
        "  # result.facial_transformation_matrixes: danh sách ma trận (1 cái mỗi mặt)\n",
        "  mats = getattr(result, \"facial_transformation_matrixes\", None) or []\n",
        "  now_s = time.perf_counter()\n",
        "\n",
        "  if mats:\n",
        "      # In 1 ma trận đầu tiên (mặt 0); reshape về (4,4) nếu cần\n",
        "      for i, Mi in enumerate(mats):\n",
        "        Mi = np.array(Mi)\n",
        "      if Mi.size == 16: Mi = Mi.reshape(4, 4)\n",
        "      print(f\"[face {i}] matrix:\\n{Mi}\")\n",
        "\n",
        "      # In ra console ~mỗi 1s để tránh spam\n",
        "      if now_s - mat_prev > 1.0:\n",
        "          np.set_printoptions(precision=4, suppress=True)\n",
        "          print(\"facial_transformation_matrixes[0]:\\n\", M0)\n",
        "          mat_prev = now_s\n",
        "  else:\n",
        "      # Không có mặt nào → có thể log thưa hơn nếu muốn\n",
        "      if now_s - log_prev > 1.0:\n",
        "          print(\"No face / no transformation matrix.\")\n",
        "          log_prev = now_s\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will also visualize the face blendshapes categories using a bar graph."
      ],
      "metadata": {
        "id": "lKelLdIfwL4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_face_blendshapes_bar_graph(detection_result.face_blendshapes[0])"
      ],
      "metadata": {
        "id": "l0id2t5Vl83m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And print the transformation matrix."
      ],
      "metadata": {
        "id": "ckKurV96cG01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(detection_result.facial_transformation_matrixes)"
      ],
      "metadata": {
        "id": "xixKF10-rmse"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}